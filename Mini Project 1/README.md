# MiniProject 1  
**Overview**  
This directory consolidates all materials, code implementations, and documentation associated with **Mini Project 1** for the **Deep Learning course (Fall 2024)**.  

**Objective**  
The primary goal of Mini Project 1 is to implement and evaluate core principles of neural networks. Students will construct **simple neural networks** and **multi-layer perceptrons (MLPs)** to address real-world classification challenges. The project emphasizes analyzing the effects of architectural choices, hyperparameter tuning (e.g., hidden layers, neuron counts, optimizers), and regularization strategies (e.g., Dropout, L2-Regularization) on model performance.  

**Key Tasks**  
- **Activation Functions**: Developing and implementing activation functions along with their gradient calculations.  
- **Neural Network Design**: Building task-specific neural architectures for classification problems.  
- **Hyperparameter Exploration**: Investigating the influence of hidden layers, neuron configurations, and optimization algorithms (e.g., SGD, Adam).  
- **Regularization Techniques**: Applying methods like Dropout and L2-Regularization to mitigate overfitting and enhance generalization.  
- **Result Interpretation**: Analyzing and visualizing model outcomes to enhance interpretability and understanding of network behavior.  

**Deliverables**  
- Code implementations for neural networks, activation functions, and training pipelines.  
- Comparative analysis of model performance across different configurations.  
- Visualizations of learning curves, decision boundaries, and loss/accuracy trends.  
- A comprehensive report detailing methodologies, experimental findings, and insights.  

This project serves as a foundational exercise to bridge theoretical concepts with practical implementation in deep learning.
